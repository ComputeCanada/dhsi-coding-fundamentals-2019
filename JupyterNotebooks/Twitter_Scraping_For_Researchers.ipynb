{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Twitter Scraping for Researchers\n",
    "\n",
    "This workbook is meant to provide some simple, working examples for researchers who would like to collect information from Twitter.  While Twitter provides their own tools and libraries for this they are a little too granular and possibly unfamiliar to many in the research community.  For this reason this workbook uses a Python library build by a third party that greatly streamlines the process of collecting tweets.\n",
    "\n",
    "The Python library used is called TwitterAPI (no space) and it can be found at https://github.com/geduldig/TwitterAPI.  Most of the code that is throughout this workbook is drawn directly from the examples on these pages.\n",
    "\n",
    "What this workbook does not do that most researchers will likely need to add to their project is show how to put the tweets that are scraped into a database.  If resesearchers are interested in writing to a database then it should be straightforward to consult the documentation available for connecting that database to Python and then substitute the appropriated commands for the sections of this workbook that write to files.\n",
    "\n",
    "On the assumption that you have a developer account and a Python environment that can run this notebook let's get started by installing the TwitterAPI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the TwitterAPI library installed on the system we should be able to open it for use throughout this workbook with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note that almost every piece of code in the remainder of this workbook assumes that the cell above has been run in advance of it being run.  If you open the workbook and immediately try to run a cell other than this one first then it is likely that you will receive an error.  Simply run the cell above and try to run the cell you want to run again.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "Use of this workbook–and the Twitter Developer API in general—requires a developer account from Twitter.  Unlike the early days of Twitter when anyone with a regular Twitter account who requested a developer account would just be given one Twitter now screens requests for developer accounts, a process that can stall getting started by many days.  If you had a developer account previously and created applications (apps) that used the Twitter application programming interfaces (APIs) then you may still be able to use these apps to do some work but it is possible that their ability to access the Twitter archive has been reduced and, if so, that you'll need to apply for a new developer account to correct this.  A developer account may be requested from https://developer.twitter.com/en/apply/user.\n",
    "\n",
    "As the [TwitterAPI Documentation](https://geduldig.github.io/TwitterAPI/authentication.html) points out: _Twitter supports both user and application authentication, called oAuth 1 and oAuth 2, respectively. User authentication gives you access to all API endpoints, basically read and write persmission. It is also required in order to using the Streaming API. Application authentication gives you access to just the read portion of the API – so, no creating or destroying tweets. Application authentication, however, has elevated rate limits._  \n",
    "We will use oAuth1 throughout this workbook even though we'll only be reading tweets since it can be used in more situations (in particular when we try to read from the streaming API).  If it is necessary to read Twitter API endpoints (other than the streaming endpoint) at a faster rate than this workbook initially provides then consider switching to oAuth2.\n",
    "\n",
    "Both authentication methods will require you to collect some information about keys and tokens and paste it into the appropriate section of the cell below.  This key and token information is generated when you create a profile for an app on the Twitter Developer site.  App profiles can be created at https://developer.twitter.com/en/apps.  That same page will hold a list of all the profiles that you have created and clicking on the \"Details\" button for each app will bring you to a summary page.  There will be a link/tab near the top of the page called \"Keys and Tokens\" and clicking this will bring you to the page with the key and token information.\n",
    "\n",
    "!!! IMPORTANT !!!\n",
    "\n",
    "If the code in the cells belows fails it is likely because you need to put in your own authentication details.\n",
    "\n",
    "!!! IMPORTANT !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oAuth1 (User Identification)\n",
    "Paste in the required key and token information from the Twitter Developer site into the cell below and then run it in order to use this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'zb64BiFI5aksYSCFj1bmZUkT5'\n",
    "api_key_secret = 'J1h1AvgYW2Ixu9U8NploawihVym4gYZqDg0iiOyuA0OYIFQNdd'\n",
    "access_token = '557113581-ySShrkeo71Wpdr1lP3I8k3HhJPTNHPUTflbT4rJz'\n",
    "access_token_secret = 'wiYe4wbO7KFzq0OPL8jM1C4HT4hnSyMHn5rNT56mZgX6B'\n",
    "\n",
    "api = TwitterAPI(api_key, \n",
    "                 api_key_secret, \n",
    "                 access_token, \n",
    "                 access_token_secret)\n",
    "\n",
    "api.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful the output of the cell above should look something like:\n",
    "\n",
    "    <requests_oauthlib.oauth1_auth.OAuth1 at 0x107b8bba8>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oAuth2 (App Identification)\n",
    "!!!WARNING!!! \n",
    "\n",
    "Pasting in the required key and token information from the Twitter Developer site into the cell below and then running it will prevent you from using the streaming endpoint, which requires oAuth1.  If you choose to try oAuth2 in the streaming example and receive the following error\n",
    "\n",
    "    TwitterRequestError: Twitter request failed (401)\n",
    "\n",
    "then simply complete the code cell in the oAuth1 section and run it.\n",
    "\n",
    "!!!WARNING!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'LYgzEZvg2lHcQyuqgaVhuEh9o'\n",
    "api_key_secret = 'OkQ33YKl9EgNqDFg3dIUBzjHD26CCixFc6ppfKxLbGdL78DRDz'\n",
    "\n",
    "api = TwitterAPI(api_key,\n",
    "                 api_key_secret,\n",
    "                 auth_type='oAuth2')\n",
    "\n",
    "api.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful the output of the cell above should look something like:\n",
    "\n",
    "    <TwitterAPI.BearerAuth.BearerAuth at 0x107b9acc0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Tweets\n",
    "Given that most people refer to tweets as bits of text that are usually 140 characters or less it is generally surprising to discover that this is only the proverbial \"tip of the iceberg\" in terms of what a tweet really is.  In this section we'll see exactly what a tweet is, how to improve looking at the full content, and then how to grab the portions that we want (usually the \"text\").\n",
    "\n",
    "To make this easy we'll only request a single tweet by its ID number.  Every tweet has its own unique ID and can be requested if that ID is known.  We request the tweet with ID# 210462857140252672 and then print the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = api.request('statuses/show/:%d' % 210462857140252672)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of running the cell above will be something like `<TwitterAPI.TwitterAPI.TwitterResponse object at 0x107b9af28>`, which isn't quite what we are looking for.  This `TwitterResponse object` is a bundle of information related to the request including status code returned (`r.status_code`), how much of your quota is left (`r.get_quota`), the response headers (`r.headers`), etc.  What we wantis the \"text\" portion of this response (`r.text`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot more than 140 characters!\n",
    "\n",
    "Exactly what is there is hard to determine though given the formatting.  We can do better.\n",
    "\n",
    "The format that this content is in is JavaScript Object Notation (JSON), which is really a nested list of properties.  Python doesn't know this is JSON though so we need to tell it.  We do this in the next cell by importing the `json` library, converting `r.text` to json using the load string method ( `.loads()` ), and then outputting that json with formatting using the output string method ( `.dumps()` ) with some some options added for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "parsed_r = json.loads(r.text)\n",
    "print(json.dumps(parsed_r, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much nicer to read, especially since the various components have been alphabetized.  Having the response text as JSON also allows us to easily access each subcomponent.  We show this in the next cell by printing the text (sometimes called the \"body\" of the tweet), the ID# of the tweet, and the screen name of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tweet Body: \",parsed_r['text'])\n",
    "print(\"Tweet ID: \",parsed_r['id'])\n",
    "print(\"Scree Name: \",parsed_r['user']['screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than parse the output to JSON everytime we can combine the Twitter Response Object's `.get_iterator()` method with a for-loop to do this directly.  It's less work overall and is cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = api.request('statuses/show/:%d' % 210462857140252672)\n",
    "for item in r.get_iterator():\n",
    "    print(\"Tweet Body: \",item['text'])\n",
    "    print(\"Tweet ID: \",item['id'])\n",
    "    print(\"Screen Name: \",item['user']['screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "There are two approaches to collecting information from Twitter: grabbing tweets as they are published and searching through the archive of past tweets.  The first approach is known as \"streaming\" and we'll look at how to use it now.  It is important to note up front that the results returned using this method are incomplete: you will _not_ necessarily capture every single tweets that you intend to this way.  Still, you can get a lot of tweets in a very short time and likely enough to get you started.\n",
    "\n",
    "Streaming amounts to applying a set of filters to the stream of all tweets and capturing what is matched by those filters.  For this first example we'll simply print the body of each tweet out on the screen.  There are lots of opinions about Donald Trump right now so we'll use 'trump' as the term we are tracking.\n",
    "\n",
    "!!! IMPORTANT !!!\n",
    "\n",
    "The cell below will run indefinitely.  You'll want to stop it at some point so be prepared to click the stop button at the top of this workbook once you have some tweets in the output cell.\n",
    "\n",
    "!!! IMPORTANT !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_TERM = 'trump'\n",
    "\n",
    "r = api.request('statuses/filter', {'track': TRACK_TERM})\n",
    "\n",
    "for item in r.get_iterator():\n",
    "    print(item['text'] if 'text' in item else item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As satisfying as it may be to have an endless stream of tweets scroll in front of you there isn't much value in it unless you are able to capture the tweets for analysis in the future.  The ideal way to do this is to write the tweets to a database because then you will have the search features of the database at your disposal.  However, as mentioned at the start of this workbook selecting and setting up a database goes beyond what will be covered here (If you're not sure where to start, [MongoDB](https://www.mongodb.com/) is a good place to start given that it's internal format is very similar to the JSON that tweets come in).\n",
    "\n",
    "Here we will simply write the text and ID number to a file.  We do this using `with` because this method of opening a file will ensure that it will close properly if the program crashes/halts unexpectedly, something that in inevitable at this point because the only way we have to stop the code we are running right now is to interrupt it.  There is some casting values to strings using the `str` function when writing to the output file because the ` .write() `  method requires a single string as an input.  Note as well the addition of the line break (`\\n`) to ensure that each tweet body starts on a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_TERM = 'trump'\n",
    "\n",
    "r = api.request('statuses/filter', {'track': TRACK_TERM})\n",
    "\n",
    "with open(\"streamTweets.csv\",\"a\") as outfile:\n",
    "    for item in r.get_iterator():\n",
    "        line = item['text'] + ',' + str(item['id'])\n",
    "        print(line if 'text' in item else item)\n",
    "        outfile.write((repr(line) + '\\n') if 'text' in item else item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output (there should be a file called \"streamTweets.csv\" in the same directory as this notebook) we can see that the body of each tweet is printed on its own line followed by a comma which is followed by the tweet ID... mostly.  Scrolling through the list will reveal that there are tweets that span multiple lines and blank spaces.  Why is this?  The body of some tweets includes line break characters (` \\n `).\n",
    "\n",
    "If we compare what was printed to the screen to what was written in the file we'll see the `\\n`'s on the screen translated to blank lines in the file.\n",
    "\n",
    "While there is an argument to be made that removing these characters makes no difference to the content of the tweet the counter argument is that line breaks are important punctuation and should be kept with the original tweet.  We should keep the line breaks.\n",
    "\n",
    "A popular way to do this would be to use a regular expression and replace each `\\n` that occurs with a `\\\\\\\\n` so that each `\\` is appropriately escaped as it is passed from the variable to the file.  The problem with this method is that there are other characters that might appear as well (either in tweets or elsewhere) and while we could write a regular expression to do the substitution in each case Python offers a better way: the [representation function](https://docs.python.org/3.5/library/functions.html#repr).  To do this we pass the line variable to the function `repr` as we write it to the output file, as in the example below.\n",
    "\n",
    "(Remember to stop the cell after a few seconds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_TERM = 'myocardial infarction'\n",
    "\n",
    "r = api.request('statuses/filter', {'track': TRACK_TERM})\n",
    "\n",
    "with open(\"sentimentTest.csv\",\"a\") as outfile:\n",
    "    for item in r.get_iterator():\n",
    "        line = item['text'] + ',' + str(item['id'])\n",
    "        print(line if 'text' in item else item)\n",
    "        outfile.write((repr(line) + '\\n') if 'text' in item else item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking streamTweets.csv shows that this approach is working well.  We won't write to an output file in every example in the rest of this workbook keep in mind that you can use the same methods in all the examples that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Search\n",
    "\n",
    "The [Standard Search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets) allows for searching in the past 7 days.  It is rate limited to 180 requests per 15 minutes using oAuth1 and 450 requests per 15 minute using oAuth2.  It is also \"not exhaustive\", meaning that the full body of tweets matching search criteria within the window is unlikely to be returned (maybe if the body of tweets is very small).\n",
    "\n",
    "We'll shift away from the politically hot topic of Donald Trump to the topic of pizza for these next examples.\n",
    "\n",
    "Note the use of the `.get_quota()` method on the response object in order to see how much of our quota remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERM = 'pizza pie'\n",
    "\n",
    "r = api.request('search/tweets', {'q': SEARCH_TERM})\n",
    "\n",
    "for item in r.get_iterator():\n",
    "    print(item['text'] if 'text' in item else item)\n",
    "\n",
    "print('\\nQUOTA: %s' % r.get_quota())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be working but there are not many tweets being returned.  We can increase this by specifying the `count` parameter.  We'll also add a simple counter just to see what we are actually getting.  \n",
    "\n",
    "The set of all the parameters that can be invoked is available [HERE](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERM = '#pizza'\n",
    "COUNT = 100 \n",
    "\n",
    "r = api.request('search/tweets', {'q': SEARCH_TERM, 'count': COUNT})\n",
    "\n",
    "a = 1\n",
    "for item in r.get_iterator():\n",
    "    print(a)\n",
    "    print(item['text'] if 'text' in item else item)\n",
    "    a=a+1\n",
    "\n",
    "print('\\nQUOTA: %s' % r.get_quota())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, not 100 tweets but certainly more than before.  To go back further we'll need to look at paging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paging \n",
    "Twitter returns results in chunks that are called \"pages\".  In the example above we are seeing just the first page of results and setting the `count` parameter set the maximum number of results to return per page.  If you want to go back further then it becomes necessary to send multiple requests to the API in succession with each one asking for the next page.  While this can be implemented \"by hand\" TwitterAPI makes this much easier by providing a paging function called 'TwitterPager' (You can read more about it [HERE](https://geduldig.github.io/TwitterAPI/paging.html) that does all of the heavy lifting for you by tracking what page to ask for, ensuring the request rate is not too high, and generally managing the connection.  It is invoked in an almost identical way to everything you have seen so far in this workbook.\n",
    "\n",
    "Unlike the previous examples where we were printing out the body of each tweet here we will print out only the date the tweet was created and the ID.  This is done simply because it makes it easier to track what is happening.\n",
    "\n",
    "It is important to note that as with the streaming API endpoint you will need to stop the code from running.  While you will eventually hit the 7-day limit it is unlikely that you want to wait that long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterPager\n",
    "\n",
    "SEARCH_TERM = 'pizza'\n",
    "COUNT = 100\n",
    "\n",
    "pager = TwitterPager(api, 'search/tweets', {'q': SEARCH_TERM, 'count': COUNT})\n",
    "\n",
    "for item in pager.get_iterator():\n",
    "    #print(item['text'] if 'text' in item else item)\n",
    "    print((item['created_at'], item['id']) if 'text' in item else item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that as the code runs the dates and the tweet IDs both roll backwards and the search tool moves from the present (tweets become accessible via the search APIs about 30 seconds after they are created) to the past.\n",
    "\n",
    "The TwitterAPI documentation provides some [advanced ways to add fault tolerance](https://geduldig.github.io/TwitterAPI/faulttolerance.html).  These are fairly sophisticated and involve checking status codes and the like.  While valuable it is inevitable that you will end up halting your program for one reason or another and need it to restart scraping where it left off.  \n",
    "\n",
    "The code below does this by first checking to see if there is an object called \"item\" that has a value keyed to 'id'.  If it does then it captures this ID and uses it as input into the TwitterPager function so that all new tweets collected will be earlier than it.  If the value does not exist then an empty string is assigned as the ID to start from which the TwitterAPI will ignore and start providing input from the present.\n",
    "\n",
    "This code will work as long as the notebook stays open, no matter how often the cell is interrupted.  If you close the notebook and reopen it then you'll need to pass in the ID value from the last line of the output file to restart in the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterPager\n",
    "\n",
    "SEARCH_TERM = 'pizza'\n",
    "COUNT = 100\n",
    "\n",
    "try:\n",
    "    SINCE_ID = item['id']\n",
    "except:\n",
    "    SINCE_ID = ''\n",
    "\n",
    "pager = TwitterPager(api, 'search/tweets', {'q': SEARCH_TERM, 'count': COUNT,'since_id':SINCE_ID})\n",
    "\n",
    "with open(\"restartTweetsTest.csv\",\"a\") as outfile:\n",
    "    for item in pager.get_iterator():\n",
    "            line = item['text'] + ',' + str(item['id'])\n",
    "            print(line if 'text' in item else item)\n",
    "            outfile.write((repr(line) + '\\n') if 'text' in item else item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premium Access\n",
    "\n",
    "Access beyond stream filtering and searching imperfectly through the past 7 days requires some extra steps beyond simply making an app.\n",
    "\n",
    "1. Setting up a dev environment.  Within the Twitter developer site click on your name in the top right corner.  From the menu select \"Dev environments\".  Follow the interface to create the environments that you would like and associate an app with each.\n",
    "2. Note the name of each dev environment because it will go into one of the variables called `LABEL`, below.  I named my 30-Day development environment \"30DayTesting\" and my full archive development environment \"fullArchiveTesting\".  So in the 30 Day example I set `LABEL` to \"30DayTesting\" and in the Full Archive example I set `LABEL` to \"fullArchiveTesting\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterAPI\n",
    "\n",
    "SEARCH_TERM = 'pizza'\n",
    "PRODUCT = '30day'\n",
    "LABEL = '30DayTesting'\n",
    "\n",
    "r = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), \n",
    "                {'query':SEARCH_TERM})\n",
    "\n",
    "for item in r:\n",
    "    print(item['text'] if 'text' in item else item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterAPI\n",
    "\n",
    "SEARCH_TERM = 'pizza'\n",
    "PRODUCT = 'fullarchive'\n",
    "LABEL = 'fullArchiveTesting'\n",
    "\n",
    "r = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), \n",
    "                {'query':SEARCH_TERM})\n",
    "\n",
    "for item in r:\n",
    "    print(item['text'] if 'text' in item else item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
